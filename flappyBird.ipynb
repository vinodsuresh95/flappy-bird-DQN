{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install flappy-bird-gymnasium\n",
    "# !pip install tensorflow \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play the game (use space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!flappy_bird_gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the enviroment, checking the returned variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import flappy_bird_gymnasium\n",
    "# import gymnasium\n",
    "# env = gymnasium.make(\"FlappyBird-v0\", render_mode=\"human\", use_lidar=False)\n",
    "\n",
    "# obs, _ = env.reset()\n",
    "# while True:\n",
    "#     # Next action:\n",
    "#     # (feed the observation to your agent here)\n",
    "    \n",
    "#     action = env.action_space.sample()\n",
    "\n",
    "#     # Processing:\n",
    "#     #obs is the next state after performing the action\n",
    "    \n",
    "#     obs, reward, terminated, _, info = env.step(action)\n",
    "    \n",
    "#     # Checking if the player is still alive\n",
    "#     if terminated:\n",
    "#         break\n",
    "\n",
    "# env.close()\n",
    "# # the numbers in obs are actually normalized between -1 and 1\n",
    "# print(\"Final Observation: \", obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install pytorch for the neural network, i used command for gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
      "Requirement already satisfied: torch in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.13.1+cpu)\n",
      "Requirement already satisfied: torchvision in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.14.1+cpu)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.13.1+cpu)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: requests in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (2.28.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchvision) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchvision) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->torchvision) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initializing hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "batchSize =  32\n",
    "epsilon= 1\n",
    "epsilonDecay= 0.9992\n",
    "epsilonMIn= 0.05\n",
    "copyRate = 10\n",
    "alpha = 0.99\n",
    "learningRate = 0.0002\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyyaml in c:\\users\\hp\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyyaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(12, 200)\n",
    "        self.fc2 = nn.Linear(200, 200)\n",
    "        self.fc3 = nn.Linear(200, 100)\n",
    "        self.fc4 = nn.Linear(100, 2)\n",
    "        self.activations = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.activations = []  # Clear previous activations\n",
    "        x = F.relu(self.fc1(x))\n",
    "        self.activations.append(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        self.activations.append(x)\n",
    "        x = self.fc3(x)\n",
    "        self.activations.append(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay buffer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#A deque is fifo, using it ensures we will not run out of memory\n",
    "#As we will be removing the oldest memory when we reach the max size\n",
    "#In general the on element is a tuple of(prev state, action, reward, next state, dead or not)\n",
    "from collections import deque\n",
    "import random\n",
    "class ReplayMemory():\n",
    "    def __init__(self, maxlen, seed=None):\n",
    "        self.memory = deque([], maxlen=maxlen)\n",
    "\n",
    "        \n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "\n",
    "    def append(self, transition):\n",
    "        self.memory.append(transition)\n",
    "    #randomly take a memeory sample with specifed size\n",
    "    def sample(self, sample_size):\n",
    "        return random.sample(self.memory, sample_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08-10 12:01:12:Started training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "c:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:159: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "08-10 12:01:15: New best reward -6.9 (-100.0%) at episode 0, saving model...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (124x180 and 12x200)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [44], line 211\u001b[0m\n\u001b[0;32m    209\u001b[0m hyperparameter_set \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflappybird\u001b[39m\u001b[38;5;124m'\u001b[39m   \n\u001b[0;32m    210\u001b[0m dql \u001b[38;5;241m=\u001b[39m Agent(hyperparameter_set\u001b[38;5;241m=\u001b[39mhyperparameter_set)\n\u001b[1;32m--> 211\u001b[0m \u001b[43mdql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [44], line 172\u001b[0m, in \u001b[0;36mAgent.run\u001b[1;34m(self, training, render)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(replayBuffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mminiBatchSize:\n\u001b[0;32m    171\u001b[0m     newBatch \u001b[38;5;241m=\u001b[39m replayBuffer\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mminiBatchSize)\n\u001b[1;32m--> 172\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdqn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargetDqn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewBatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m     epsilons\u001b[38;5;241m.\u001b[39mappend(epsilon)\n\u001b[0;32m    174\u001b[0m     epsilon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(epsilon \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_decay, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon_min)\n",
      "Cell \u001b[1;32mIn [44], line 199\u001b[0m, in \u001b[0;36mAgent.train\u001b[1;34m(self, dqn, targetDqn, batch)\u001b[0m\n\u001b[0;32m    195\u001b[0m terminations \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(terminations)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;66;03m# Calculate target Q values (expected returns)\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m     targetQ \u001b[38;5;241m=\u001b[39m rewards \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m terminations) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscountFactor \u001b[38;5;241m*\u001b[39m \u001b[43mtargetDqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnewStates\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    200\u001b[0m currQ \u001b[38;5;241m=\u001b[39m dqn(states)\u001b[38;5;241m.\u001b[39mgather(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, index\u001b[38;5;241m=\u001b[39mactions\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m    201\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlossF(currQ, targetQ)\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [40], line 16\u001b[0m, in \u001b[0;36mDQN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivations \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# Clear previous activations\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivations\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[0;32m     18\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (124x180 and 12x200)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import yaml\n",
    "import flappy_bird_gymnasium\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "DATE_FORMAT = \"%m-%d %H:%M:%S\"\n",
    "\n",
    "RUNS_DIR = \"runs\"\n",
    "os.makedirs(RUNS_DIR, exist_ok=True)\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#window = PygameWindow(window_title=\"MLP Visualization\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, hyperparameter_set):\n",
    "        with open('hyperparameters.yml', 'r') as file:\n",
    "            all_hyperparameter_sets = yaml.safe_load(file)\n",
    "            hyperparameters = all_hyperparameter_sets[hyperparameter_set]\n",
    "        self.hyperparameter_set = hyperparameter_set\n",
    "\n",
    "        # Hyperparameters, can adjust from yml file\n",
    "        self. envId = hyperparameters['envId']\n",
    "        self.learningRatea = hyperparameters['learningRatea'] \n",
    "        self.discountFactor = hyperparameters['discountFactorg']\n",
    "        self.networkSyncRate = hyperparameters['networkSyncRate'] \n",
    "        \n",
    "        self.epsilon_init = hyperparameters['epsilonInit']  \n",
    "        self.epsilon_decay = hyperparameters['epsilonDecay']  \n",
    "        self.epsilon_min = hyperparameters['epsilonMin'] \n",
    "        self.stop_on_reward = hyperparameters['stopOnReward']  \n",
    "        self.env_make_params = hyperparameters.get('env_make_params', {})  \n",
    "        self.lossF = nn.MSELoss()\n",
    "        self.optimizer = None\n",
    "        self.LOG_FILE = os.path.join(RUNS_DIR, f'{self.hyperparameter_set}.log')\n",
    "        self.MODEL_FILE = os.path.join(RUNS_DIR, f'{self.hyperparameter_set}.pt')\n",
    "        \n",
    "\n",
    "        self.replayMemorySize = hyperparameters['replayMemorySize']  \n",
    "        self.miniBatchSize = hyperparameters['miniBatchSize']\n",
    "\n",
    "    def run(self, training=True, render=False):\n",
    "        if training:\n",
    "            startTime = datetime.now()\n",
    "            \n",
    "\n",
    "            log_message = f\"{startTime.strftime(DATE_FORMAT)}:Started training\"\n",
    "            print(log_message)\n",
    "            with open(self.LOG_FILE, 'w') as file:\n",
    "                file.write(log_message + '\\n')\n",
    "\n",
    "        env = gym.make(self. envId, render_mode='human' if render else None, **self.env_make_params)\n",
    "        statesCount = env.observation_space.shape[0]\n",
    "        rewards = []\n",
    "        actionsCount = env.action_space.n\n",
    "\n",
    "        dqn = DQN().to(device)\n",
    "        if training:\n",
    "            replayBuffer = ReplayMemory(self.replayMemorySize)\n",
    "            steps = 0\n",
    "            epsilons = []\n",
    "            epsilon = self.epsilon_init\n",
    "            maxReward = -999999\n",
    "            # creating the target network, initializing to be the same as training one\n",
    "            targetDqn = DQN().to(device)\n",
    "            self.optimizer = torch.optim.Adam(dqn.parameters(), lr=self.learningRatea)\n",
    "            targetDqn.load_state_dict(dqn.state_dict())\n",
    "        else:\n",
    "            # Load learned policy\n",
    "            dqn.load_state_dict(torch.load(self.MODEL_FILE, map_location=torch.device('cpu')))\n",
    "\n",
    "            # switch model to evaluation mode\n",
    "            dqn.eval()\n",
    "\n",
    "        # train indefinitely until results are satisfying enough\n",
    "        for episode in itertools.count():\n",
    "            terminated = False\n",
    "            state, _ = env.reset()\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "            epReward = 0.0\n",
    "            biasedEpisodes = 0\n",
    "            while not terminated and epReward < self.stop_on_reward:\n",
    "                # if the random number, between 0,1, is less than epsilon, take a random action\n",
    "                # now if we will be greedy(exploit), then we will take the action with the highest q value from the network\n",
    "                if training:\n",
    "                    if episode < biasedEpisodes:\n",
    "                        # Apply biased action selection for the first 'biased_episodes' episodes\n",
    "                        if random.random() < epsilon:\n",
    "                            # Bias the random action selection towards action 0\n",
    "                            if random.random() < 0.9:  # 90% probability for action 0\n",
    "                                action = torch.tensor(0).to(device)\n",
    "                            else:\n",
    "                                action = torch.tensor(1).to(device)\n",
    "                        else:\n",
    "                            with torch.no_grad():\n",
    "                                action = dqn(state.unsqueeze(dim=0)).squeeze().argmax()\n",
    "                    else:\n",
    "                        # Regular epsilon-greedy action selection after 'biased_episodes' episodes\n",
    "                        if random.random() < epsilon:\n",
    "                            action = env.action_space.sample()\n",
    "                            with open('output.txt', 'w') as file:\n",
    "                                file.write(str(action.item()))\n",
    "                            action = torch.tensor(action).to(device)\n",
    "                        else:\n",
    "                            with torch.no_grad():\n",
    "                                action = dqn(state.unsqueeze(dim=0)).squeeze().argmax()\n",
    "                                with open('output.txt', 'w') as file:\n",
    "                                    file.write(str(action.item()))\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        action = dqn(state.unsqueeze(dim=0)).squeeze().argmax()\n",
    "                        \n",
    "                \n",
    "                        \n",
    "\n",
    "                # Processing:\n",
    "                # obs is the next state after performing the action\n",
    "                # giving the network to the training device\n",
    "\n",
    "                newState, reward, terminated, _, info = env.step(action.item())\n",
    "                \n",
    "                # Convert newState to a string format suitable for saving to a file\n",
    "                newState_str = ' '.join(map(str, newState))\n",
    "\n",
    "                # Save to input.txt, overwriting previous content\n",
    "                with open('input.txt', 'w') as file:\n",
    "                    file.write(newState_str)\n",
    "                newState = torch.tensor(newState, dtype=torch.float32).to(device)\n",
    "                reward = torch.tensor(reward, dtype=torch.float32).to(device)\n",
    "                epReward += reward\n",
    "\n",
    "                # after taking an action save the tuple in the replay buffer\n",
    "                if training:\n",
    "                    replayBuffer.append((state, action, newState, reward, terminated))\n",
    "                    steps += 1\n",
    "                # update the state, so that when saving the next action tuple the vars dont get mixed up\n",
    "                state = newState\n",
    "\n",
    "            rewards.append(epReward)\n",
    "            if training:\n",
    "                if epReward > maxReward:\n",
    "                    log_message = f\"{datetime.now().strftime(DATE_FORMAT)}: New best reward {epReward:0.1f} ({(epReward - maxReward) / maxReward * 100:+.1f}%) at episode {episode}, saving model...\"\n",
    "                    print(log_message)\n",
    "                    with open(self.LOG_FILE, 'a') as file:\n",
    "                        file.write(log_message + '\\n')\n",
    "\n",
    "                    torch.save(dqn.state_dict(), self.MODEL_FILE)\n",
    "                    maxReward = epReward\n",
    "\n",
    "                \n",
    "                current_time = datetime.now()\n",
    "                \n",
    "                # applying a decay to the current epsilon to encourage exploitation\n",
    "                # cannot go under epsilonMin so there will always be some sort of exploration\n",
    "\n",
    "                if len(replayBuffer) > self.miniBatchSize:\n",
    "                    newBatch = replayBuffer.sample(self.miniBatchSize)\n",
    "                    self.train(dqn, targetDqn, newBatch)\n",
    "                    epsilons.append(epsilon)\n",
    "                    epsilon = max(epsilon * self.epsilon_decay, self.epsilon_min)\n",
    "                    # i could have used modulus but i am afraid steps can get too big and overflow\n",
    "                    # anyway after a certain number of steps, copy the training network to the target network\n",
    "                    # i believe it is best keeping a low rate as we do not want chasing a bad target for long\n",
    "                    # another thing is, how about increasing the rate as we go\n",
    "                    if steps > self.networkSyncRate:\n",
    "                        targetDqn.load_state_dict(dqn.state_dict())\n",
    "                        steps = 0\n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "    def train(self, dqn, targetDqn, batch):\n",
    "        # Transpose the list of experiences and separate each element\n",
    "        states, actions, new_states, rewards, terminations = zip(*batch)\n",
    "\n",
    "        # Stack tensors to create batch tensors\n",
    "        states = torch.stack(states)\n",
    "        actions = torch.stack(actions)\n",
    "        newStates = torch.stack(new_states)\n",
    "        rewards = torch.stack(rewards)\n",
    "        terminations = torch.tensor(terminations).float().to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Calculate target Q values (expected returns)\n",
    "            targetQ = rewards + (1 - terminations) * self.discountFactor * targetDqn(newStates).max(dim=1)[0]\n",
    "        currQ = dqn(states).gather(dim=1, index=actions.unsqueeze(dim=1)).squeeze()\n",
    "        loss = self.lossF(currQ, targetQ)\n",
    "\n",
    "    \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()  \n",
    "        self.optimizer.step()  \n",
    "\n",
    "\n",
    "hyperparameter_set = 'flappybird'   \n",
    "dql = Agent(hyperparameter_set=hyperparameter_set)\n",
    "dql.run(training=False, render=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mzmq.error.ZMQError: Address in use. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
